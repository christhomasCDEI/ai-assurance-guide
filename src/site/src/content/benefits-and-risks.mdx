---
title: Benefits and risks
---

import CookieBanner from "../components/cookies"

Data-driven technologies, such as artificial intelligence (AI), have the potential to bring about significant benefits for our economy and society. AI has been harnessed to help us combat the pandemic; from enabling the discovery of new vaccines and rapid virus detection methods, to powering dashboards that help clinicians to make crucial treatment decisions on the frontline. In 2020 DeepMind’s deep-learning programme AlphaFold made a huge leap forward in solving one of biology's greatest challenges, the <OutboundLink href="https://www.nature.com/articles/d41586-020-03348-4">protein folding problem</OutboundLink> - how to determine a protein's 3-dimensional shape from its amino acid sequence. This breakthrough could vastly accelerate efforts to understand the building blocks of cells and could improve and speed up drug discovery.

AI presents game changing opportunities in other sectors, too, through the potential for operating an efficient <OutboundLink href="https://www.weforum.org/agenda/2021/03/artificial-intelligence-is-key-to-grid-resilience/">green energy grid</OutboundLink>, <OutboundLink href="https://www.gov.uk/government/publications/the-role-of-ai-in-addressing-misinformation-on-social-media-platforms">tackling misinformation</OutboundLink> on social media platforms.

However, AI systems also introduce risks that need to be managed. The <OutboundLink href="https://link.springer.com/content/pdf/10.1007/s11948-021-00319-4.pdf">autonomous, complex and scalable</OutboundLink> nature of AI systems (in particular, machine learning) pose risks beyond that of regular software. These features pose fundamental challenges to our existing methods for assessing and mitigating the risks of using digital technologies.

The autonomous nature of AI systems makes it difficult to assign accountability to individuals if harms occur; the complexity of AI systems often prevents users or affected individuals from understanding the link between a system’s output or decision and its causes, providing further challenges to assigning accountability; and the scalability of AI makes it particularly difficult to define legitimate values and governance frameworks for a system’s operation e.g. across social contexts or national jurisdictions.

As these technologies are more widely adopted, there is an increasing need for a range of actors, including regulators, developers, executives, and frontline users, to check that these tools are functioning as expected and to demonstrate this to others. 

However, these actors often lack the information or specialist knowledge to do this. To address this gap, effective AI assurance is required to enable these actors to make informed judgements about these systems .

# Heading 1

## Heading 2

### Heading 3
