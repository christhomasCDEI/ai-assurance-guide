---
title: Challenges for AI assurance
---

import CookieBanner from "../components/cookies"
import Collapse from "../components/collapse"
import OutboundLink from "../components/outbound-link"
import License from "../components/license"

<CookieBanner />

## A mature ecosystem will require ongoing effort

We have deliberately used the word ecosystem to describe the network of roles needed for assurance. These are interdependent roles that co-evolve, and rely on a balance between competing interests between different actors.

There are some fundamental tensions of assurance ecosystems that can be managed, but do not go away even in mature ecosystems, such as gaming, conflicts of interest, the power dynamics behind technical standards making, balancing innovation and risk minimisation, and responsibility for good AI decisions.

TODO: add image

Tension 1 lies between **regulators who may want or need specific rules or requirements** that they can enforce, and the **government, which may be intentionally hands-off** when it comes to setting those standards, or setting policy objectives that drive those standards. Governments may be hesitant to set specific rules for two understandable reasons. Firstly, if they set them too prematurely, they might set the bar too low or be informed by popular sentiment rather than by careful consideration. Secondly, there may be aspects of regulation that government feels is beyond the scope of the state. For example, there are certain aspects of codifying fairness which might need to be left to individual judgement. 

Tension 2 arises due to different **trade-offs between risk minimisation and encouraging innovation.** Governments, regulators, developers, and executives each face a different balance of the risks and benefits of these technologies, and often have incentives that are in tension. To resolve this, a balanced approach is needed to consider the risks and benefits across the ecosystem. The ideal arrangement is one where assurance acts to give greater confidence to both developers and purchasers of AI systems and enables wider, safer adoption. 

Tension 3 concerns **accepting responsibility for good AI decisions.** Developers and executives may have the same goal - for example to have fair and safe outcomes - but each would like the other party to ultimately be accountable. Developers may say that they are creating a tool and it’s up to executives to use it correctly. Executives may say they are using a tool and it’s up to developers to make it work well. In practice, both may be responsible, but more guidance will be necessary to resolve this tension. This tension is particularly pertinent in the AI context where executives often procure AI technology rather than building tools in-house.

Tension 4, ‘gaming’ the outcome, might arise if developers and executives offer too much transparency about how the system works, potentially creating a situation where citizens or frontline users can **tweak the inputs to achieve certain outcomes,** thereby ‘gaming’ the system. This might be considered a downside of <OutboundLink href="https://arxiv.org/abs/1708.01870">too much transparency</OutboundLink>.

Tension 5 might arise where those who deploy algorithms need to communicate trustworthiness with affected individuals, and likewise affected individuals need assurance that an algorithmic system makes good decisions. These parallel goals - one to communicate trustworthiness, and the other to entrust - may lead to **situations where developers and frontline users offer empty explanations** and transparency to <OutboundLink href="https://arxiv.org/abs/1708.01870">psychologically soothe</OutboundLink>, or in a way that may ultimately be meaningless to consumers and citizens.


**Because of these fundamental tensions, even mature ecosystems can fail,** as we have seen with the <OutboundLink href="https://www.bbc.co.uk/news/business-34324772">Volkswagen emissions scandal</OutboundLink>, where the Environmental Protection Agency found that VW had programmed their cars to display false emissions figures in testing. The <OutboundLink href="https://www.investopedia.com/articles/economics/09/financial-crisis-review.asp">2008 financial crisis</OutboundLink> provides a further example, where credit rating agencies falsely gave the highest triple-A ratings to mortgage backed securities, ignoring correlated risk and leaving financial institutions with near-worthless investments in subprime mortgages. Though a well functioning assurance ecosystem makes these the exception rather than the rule, preventing and mitigating these challenges requires vigilance and scrutiny, and cannot be solved by a ‘set-and-forget’ testing regime. 

Preventing and mitigating these challenges requires vigilance and scrutiny, and cannot be solved by a ‘set-and-forget’ testing regime. Additional challenges are posed to AI assurance by the autonomous, complex and scalable nature of AI, which make it more difficult to keep track of who is accountable, where they are accountable and what they are accountable for.

Despite the presence of tensions, an assurance ecosystem needs to be built on effective coordination between different roles. An ecosystem that relies too heavily on the role of one player might work for a while, but will be unstable due to the different incentives of the various roles. Whether that is a central regulator or reliance on the self assessed reports of developers, overreliance on one player will fail to reflect the needs and interests of actors across the ecosystem.

If only central regulators are relied on, capacity for assurance will be too low to keep up with demand, hindering innovation. Similarly, If assurance is over-reliant upon the self-assessment of developers, the ecosystem will lack the supporting structures that determine good practice and build trust and trustworthiness.

**It will be important for all actors in AI Assurance to play their part in building a trusted and trustworthy AI assurance ecosystem.**

<License />
